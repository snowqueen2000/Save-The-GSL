{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:26: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  october_snow = snow_water.loc[:30].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  november_snow = snow_water.loc[31:60].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  december_snow = snow_water.loc[61:91].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:29: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  january_snow = snow_water.loc[92:122].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:30: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  february_snow = snow_water.loc[124:151].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:31: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  march_snow = snow_water.loc[152:182].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:32: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  april_snow = snow_water.loc[183:212].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  may_snow = snow_water.loc[213:243].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  june_snow = snow_water.loc[244:273].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  july_snow = snow_water.loc[274:304].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:36: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  august_snow = snow_water.loc[305:334].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/917283276.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  september_snow = snow_water.loc[335:365].mean()\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd \n",
    "import math as m\n",
    "import statsmodels.formula.api as sm\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "IMPORT DATA AND POPULATE INTO DATAFRAMES\n",
    "   \n",
    "format is datetime (month/20/year) and data type for monthly,\n",
    "        datetime (01/01/year) and data type for annual\n",
    "\n",
    "\n",
    "data to be imported    \n",
    "    - snow water equivalent (snowpack) [inches]\n",
    "    - lake level [units??]]\n",
    "    - precipitation (salt lake metropolitan area) [inches]\n",
    "    - gdp of utah (proxy for commercial water usage) [dollars]\n",
    "    - population of utah (proxy for individual water usage)[people]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "###   IMPORT SNOW WATER EQUIVALENT DATA (inches)\n",
    "snow_water = pd.read_csv('state_of_utah_snow_water.csv') # snow water equivalent\n",
    "\n",
    "#find the mean swe per month\n",
    "october_snow = snow_water.loc[:30].mean()\n",
    "november_snow = snow_water.loc[31:60].mean()\n",
    "december_snow = snow_water.loc[61:91].mean()\n",
    "january_snow = snow_water.loc[92:122].mean()\n",
    "february_snow = snow_water.loc[124:151].mean()\n",
    "march_snow = snow_water.loc[152:182].mean()\n",
    "april_snow = snow_water.loc[183:212].mean()\n",
    "may_snow = snow_water.loc[213:243].mean()\n",
    "june_snow = snow_water.loc[244:273].mean()\n",
    "july_snow = snow_water.loc[274:304].mean()\n",
    "august_snow = snow_water.loc[305:334].mean()\n",
    "september_snow = snow_water.loc[335:365].mean()\n",
    "\n",
    "#function to convert swe data to date time\n",
    "    #one piece of data per month that is on the 20th\n",
    "def populate_snow_datetime(data, month, start_year, df):\n",
    "    for i in range(len(data)-10):\n",
    "        temp = pd.DataFrame([[datetime.date(start_year+i,month,20), data[i]]],columns=['date','swe'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#populate into a data frame\n",
    "swe_df = pd.DataFrame(columns=['date','swe'])\n",
    "\n",
    "swe_df = populate_snow_datetime(october_snow, 10, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(november_snow, 11, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(december_snow, 12, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(january_snow, 1, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(february_snow, 2, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(march_snow, 3, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(april_snow, 4, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(may_snow, 5, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(june_snow, 6, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(july_snow, 7, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(august_snow, 8, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(september_snow, 9, 1981, swe_df)\n",
    "\n",
    "swe_df = swe_df.sort_values(by='date')\n",
    "\n",
    "###   IMPORT LAKE LEVEL DATA \n",
    "lake_level = pd.read_csv('monthly', sep = '\\t', comment = '#') \n",
    "\n",
    "#function to convert lake data to datetime\n",
    "def populate_lake_datetime(data, df):\n",
    "    for i in range(len(data)-1):\n",
    "        temp = pd.DataFrame([[datetime.date(int(data['year_nu'][i+1]),int(data['month_nu'][i+1]),20), float(data['mean_va'][i+1])]],columns=['date','lake_level'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "lk_lvl_df = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_df = populate_lake_datetime(lake_level, lk_lvl_df)\n",
    "\n",
    "###   IMPORT PRECIPITATION DATA (inches)\n",
    "precipitation = pd.read_csv('precipitation_data.csv')\n",
    "\n",
    "#function to convert pecipitation data to datetime\n",
    "def populate_precip_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(12):\n",
    "            month = (str)(j+1)\n",
    "            temp = pd.DataFrame([[datetime.date(int(data['Year'][i]),int(j+1),20), float(data[month][i])]],columns=['date','precipitation'])\n",
    "            df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "precip_df = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_df = populate_precip_datetime(precipitation, precip_df)\n",
    "\n",
    "###   IMPORT GDP DATA (some form of dollars)\n",
    "gdp = pd.read_csv('UTNGSP.csv')\n",
    "\n",
    "#function to convert gdp data to datetime\n",
    "def populate_gdp_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        year = data['DATE'][i][:4]\n",
    "        temp = pd.DataFrame([[datetime.date(int(year),int(1),20), float(data['UTNGSP'][i])]],columns=['date','gdp'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "gdp_df = pd.DataFrame(columns=['date','gdp'])\n",
    "gdp_df = populate_gdp_datetime(gdp, gdp_df)\n",
    "\n",
    "###   IMPORT POPULATION GROWTH DATA\n",
    "pop_growth = pd.read_csv('population_growth.csv')\n",
    "\n",
    "def populate_pop_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        temp = pd.DataFrame([[datetime.date(int(pop_growth['year'][i]),int(1),20), float(data['pop'][i])]],columns=['date','population'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "pop_df = pd.DataFrame(columns=['date','population'])\n",
    "pop_df = populate_pop_datetime(pop_growth, pop_df)\n",
    "\n",
    "###   SLICE ALL DFS TO 1990\n",
    "swe_monthly = swe_df.loc[swe_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "precip_monthly = precip_df.loc[precip_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "lk_lvl_monthly = lk_lvl_df.loc[lk_lvl_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "gdp_annual = gdp_df.loc[gdp_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "pop_annual = pop_df.loc[pop_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "\n",
    "#reset indexes\n",
    "swe_monthly = swe_monthly.reset_index(drop=True)\n",
    "precip_monthly = precip_monthly.reset_index(drop=True)\n",
    "lk_lvl_monthly = lk_lvl_monthly.reset_index(drop=True)\n",
    "gdp_annual = gdp_annual.reset_index(drop=True)\n",
    "pop_annual = pop_annual.reset_index(drop=True)\n",
    "\n",
    "###    ANNUAL INFORMATION FOR MONTHLY DATA\n",
    "\n",
    "#function to sum months to get annual data from monthly data\n",
    "def make_annual_sum(df, str, df_annual):\n",
    "    curr_year = 1990\n",
    "    curr_inches = 0\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].year)==curr_year):\n",
    "            curr_inches += df[str][i]\n",
    "        elif(month>12 or i == len(df)):\n",
    "            temp = pd.DataFrame([[datetime.date(int(curr_year),int(1),20), float(curr_inches)]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "            curr_year += 1\n",
    "            curr_inches = 0\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "swe_annual = pd.DataFrame(columns=['date','swe'])\n",
    "swe_annual = make_annual_sum(swe_monthly, 'swe', swe_annual)\n",
    "\n",
    "precip_annual = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_annual = make_annual_sum(precip_monthly, 'precipitation', precip_annual)\n",
    "\n",
    "def make_annual_lake_level(df, str, df_annual):\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].month)==1):\n",
    "            temp = pd.DataFrame([[df['date'][i], float(df[str][i])]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "lk_lvl_annual = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_annual = make_annual_lake_level(lk_lvl_monthly, 'lake_level', lk_lvl_annual)\n",
    "\n",
    "\n",
    "###    CLEAN DATA\n",
    "\n",
    "#monthly\n",
    "swe_monthly \n",
    "precip_monthly \n",
    "lk_lvl_monthly \n",
    "\n",
    "#annual\n",
    "gdp_annual \n",
    "pop_annual\n",
    "precip_annual\n",
    "lk_lvl_annual \n",
    "swe_annual\n",
    "\n",
    "###    REGRESSION ANALYSIS\n",
    "\n",
    "final = gdp_annual.merge(pop_annual, on = 'date', how = 'outer').merge(precip_annual, on = 'date', how = 'outer').merge(lk_lvl_annual, on = 'date', how = 'outer').merge(swe_annual, on = 'date', how = 'outer')\n",
    "final['ln_gdp'] = np.log(final['gdp'])\n",
    "final['ln_pop'] = np.log(final['population'])\n",
    "final['ln_precip'] = np.log(final['precipitation'])\n",
    "final['ln_lk_lvl'] = np.log(final['lake_level'])\n",
    "final['ln_swe'] = np.log(final['swe'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>lake_level</td>    <th>  R-squared:         </th> <td>   0.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   18.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Apr 2023</td> <th>  Prob (F-statistic):</th> <td>1.72e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:03:45</td>     <th>  Log-Likelihood:    </th> <td> -45.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    25</td>      <th>  AIC:               </th> <td>   100.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   106.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td> 4228.4223</td> <td>    7.699</td> <td>  549.195</td> <td> 0.000</td> <td> 4212.362</td> <td> 4244.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>swe</th>           <td>   -0.0422</td> <td>    0.020</td> <td>   -2.154</td> <td> 0.044</td> <td>   -0.083</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>precipitation</th> <td>   -0.1253</td> <td>    0.116</td> <td>   -1.084</td> <td> 0.291</td> <td>   -0.367</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>population</th>    <td>-1.275e-05</td> <td>  4.2e-06</td> <td>   -3.034</td> <td> 0.007</td> <td>-2.15e-05</td> <td>-3.98e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gdp</th>           <td> 4.308e-05</td> <td> 3.43e-05</td> <td>    1.258</td> <td> 0.223</td> <td>-2.84e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.300</td> <th>  Durbin-Watson:     </th> <td>   0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.522</td> <th>  Jarque-Bera (JB):  </th> <td>   0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.146</td> <th>  Prob(JB):          </th> <td>   0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.098</td> <th>  Cond. No.          </th> <td>6.40e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.4e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             lake_level   R-squared:                       0.787\n",
       "Model:                            OLS   Adj. R-squared:                  0.744\n",
       "Method:                 Least Squares   F-statistic:                     18.45\n",
       "Date:                Wed, 19 Apr 2023   Prob (F-statistic):           1.72e-06\n",
       "Time:                        10:03:45   Log-Likelihood:                -45.014\n",
       "No. Observations:                  25   AIC:                             100.0\n",
       "Df Residuals:                      20   BIC:                             106.1\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "Intercept      4228.4223      7.699    549.195      0.000    4212.362    4244.483\n",
       "swe              -0.0422      0.020     -2.154      0.044      -0.083      -0.001\n",
       "precipitation    -0.1253      0.116     -1.084      0.291      -0.367       0.116\n",
       "population    -1.275e-05    4.2e-06     -3.034      0.007   -2.15e-05   -3.98e-06\n",
       "gdp            4.308e-05   3.43e-05      1.258      0.223   -2.84e-05       0.000\n",
       "==============================================================================\n",
       "Omnibus:                        1.300   Durbin-Watson:                   0.708\n",
       "Prob(Omnibus):                  0.522   Jarque-Bera (JB):                0.937\n",
       "Skew:                           0.146   Prob(JB):                        0.626\n",
       "Kurtosis:                       2.098   Cond. No.                     6.40e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.4e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "first_reg = sm.ols('lake_level ~ swe + precipitation + population + gdp', data = final).fit()\n",
    "first_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>ln_lk_lvl</td>    <th>  R-squared:         </th> <td>   0.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Apr 2023</td> <th>  Prob (F-statistic):</th> <td>1.18e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:30:21</td>     <th>  Log-Likelihood:    </th> <td>  164.02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    25</td>      <th>  AIC:               </th> <td>  -318.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>  -311.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    8.4235</td> <td>    0.048</td> <td>  175.622</td> <td> 0.000</td> <td>    8.323</td> <td>    8.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln_swe</th>    <td>   -0.0005</td> <td>    0.000</td> <td>   -2.165</td> <td> 0.043</td> <td>   -0.001</td> <td>-1.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln_precip</th> <td>   -0.0005</td> <td>    0.000</td> <td>   -1.448</td> <td> 0.163</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln_gdp</th>    <td>    0.0002</td> <td>    0.002</td> <td>    0.118</td> <td> 0.907</td> <td>   -0.003</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln_pop</th>    <td>   -0.0055</td> <td>    0.005</td> <td>   -1.203</td> <td> 0.243</td> <td>   -0.015</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.424</td> <th>  Durbin-Watson:     </th> <td>   0.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.491</td> <th>  Jarque-Bera (JB):  </th> <td>   0.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.105</td> <th>  Prob(JB):          </th> <td>   0.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.067</td> <th>  Cond. No.          </th> <td>1.22e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.22e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              ln_lk_lvl   R-squared:                       0.795\n",
       "Model:                            OLS   Adj. R-squared:                  0.754\n",
       "Method:                 Least Squares   F-statistic:                     19.38\n",
       "Date:                Wed, 19 Apr 2023   Prob (F-statistic):           1.18e-06\n",
       "Time:                        10:30:21   Log-Likelihood:                 164.02\n",
       "No. Observations:                  25   AIC:                            -318.0\n",
       "Df Residuals:                      20   BIC:                            -311.9\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      8.4235      0.048    175.622      0.000       8.323       8.524\n",
       "ln_swe        -0.0005      0.000     -2.165      0.043      -0.001   -1.92e-05\n",
       "ln_precip     -0.0005      0.000     -1.448      0.163      -0.001       0.000\n",
       "ln_gdp         0.0002      0.002      0.118      0.907      -0.003       0.004\n",
       "ln_pop        -0.0055      0.005     -1.203      0.243      -0.015       0.004\n",
       "==============================================================================\n",
       "Omnibus:                        1.424   Durbin-Watson:                   0.766\n",
       "Prob(Omnibus):                  0.491   Jarque-Bera (JB):                0.953\n",
       "Skew:                           0.105   Prob(JB):                        0.621\n",
       "Kurtosis:                       2.067   Cond. No.                     1.22e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.22e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reg = sm.ols('ln_lk_lvl ~ ln_swe + ln_precip + ln_gdp + ln_pop', data = final).fit()\n",
    "final_reg.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
