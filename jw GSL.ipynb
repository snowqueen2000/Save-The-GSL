{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:26: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  october_snow = snow_water.loc[:30].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:27: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  november_snow = snow_water.loc[31:60].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:28: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  december_snow = snow_water.loc[61:91].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:29: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  january_snow = snow_water.loc[92:122].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:30: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  february_snow = snow_water.loc[124:151].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:31: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  march_snow = snow_water.loc[152:182].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:32: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  april_snow = snow_water.loc[183:212].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:33: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  may_snow = snow_water.loc[213:243].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:34: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  june_snow = snow_water.loc[244:273].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:35: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  july_snow = snow_water.loc[274:304].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:36: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  august_snow = snow_water.loc[305:334].mean()\n",
      "/var/folders/hs/0cnj9f054qz555djc4yg9mbh0000gn/T/ipykernel_80848/681696299.py:37: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  september_snow = snow_water.loc[335:365].mean()\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd \n",
    "import math as m\n",
    "import statsmodels.formula.api as sm\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "IMPORT DATA AND POPULATE INTO DATAFRAMES\n",
    "   \n",
    "format is datetime (month/20/year) and data type for monthly,\n",
    "        datetime (01/01/year) and data type for annual\n",
    "\n",
    "\n",
    "data to be imported    \n",
    "    - snow water equivalent (snowpack) [inches]\n",
    "    - lake level [units??]]\n",
    "    - precipitation (salt lake metropolitan area) [inches]\n",
    "    - gdp of utah (proxy for commercial water usage) [dollars]\n",
    "    - population of utah (proxy for individual water usage)[people]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "###   IMPORT SNOW WATER EQUIVALENT DATA (inches)\n",
    "snow_water = pd.read_csv('state_of_utah_snow_water.csv') # snow water equivalent\n",
    "\n",
    "#find the mean swe per month\n",
    "october_snow = snow_water.loc[:30].mean()\n",
    "november_snow = snow_water.loc[31:60].mean()\n",
    "december_snow = snow_water.loc[61:91].mean()\n",
    "january_snow = snow_water.loc[92:122].mean()\n",
    "february_snow = snow_water.loc[124:151].mean()\n",
    "march_snow = snow_water.loc[152:182].mean()\n",
    "april_snow = snow_water.loc[183:212].mean()\n",
    "may_snow = snow_water.loc[213:243].mean()\n",
    "june_snow = snow_water.loc[244:273].mean()\n",
    "july_snow = snow_water.loc[274:304].mean()\n",
    "august_snow = snow_water.loc[305:334].mean()\n",
    "september_snow = snow_water.loc[335:365].mean()\n",
    "\n",
    "#function to convert swe data to date time\n",
    "    #one piece of data per month that is on the 20th\n",
    "def populate_snow_datetime(data, month, start_year, df):\n",
    "    for i in range(len(data)-10):\n",
    "        temp = pd.DataFrame([[datetime.date(start_year+i,month,20), data[i]]],columns=['date','swe'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#populate into a data frame\n",
    "swe_df = pd.DataFrame(columns=['date','swe'])\n",
    "\n",
    "swe_df = populate_snow_datetime(october_snow, 10, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(november_snow, 11, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(december_snow, 12, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(january_snow, 1, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(february_snow, 2, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(march_snow, 3, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(april_snow, 4, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(may_snow, 5, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(june_snow, 6, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(july_snow, 7, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(august_snow, 8, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(september_snow, 9, 1981, swe_df)\n",
    "\n",
    "swe_df = swe_df.sort_values(by='date')\n",
    "\n",
    "###   IMPORT LAKE LEVEL DATA \n",
    "lake_level = pd.read_csv('monthly', sep = '\\t', comment = '#') \n",
    "\n",
    "#function to convert lake data to datetime\n",
    "def populate_lake_datetime(data, df):\n",
    "    for i in range(len(data)-1):\n",
    "        temp = pd.DataFrame([[datetime.date(int(data['year_nu'][i+1]),int(data['month_nu'][i+1]),20), float(data['mean_va'][i+1])]],columns=['date','lake_level'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "lk_lvl_df = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_df = populate_lake_datetime(lake_level, lk_lvl_df)\n",
    "\n",
    "###   IMPORT PRECIPITATION DATA (inches)\n",
    "precipitation = pd.read_csv('precipitation_data.csv')\n",
    "\n",
    "#function to convert pecipitation data to datetime\n",
    "def populate_precip_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(12):\n",
    "            month = (str)(j+1)\n",
    "            temp = pd.DataFrame([[datetime.date(int(data['Year'][i]),int(j+1),20), float(data[month][i])]],columns=['date','precipitation'])\n",
    "            df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "precip_df = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_df = populate_precip_datetime(precipitation, precip_df)\n",
    "\n",
    "###   IMPORT GDP DATA (some form of dollars)\n",
    "gdp = pd.read_csv('UTNGSP.csv')\n",
    "\n",
    "#function to convert gdp data to datetime\n",
    "def populate_gdp_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        year = data['DATE'][i][:4]\n",
    "        temp = pd.DataFrame([[datetime.date(int(year),int(1),20), float(data['UTNGSP'][i])]],columns=['date','gdp'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "gdp_df = pd.DataFrame(columns=['date','gdp'])\n",
    "gdp_df = populate_gdp_datetime(gdp, gdp_df)\n",
    "\n",
    "###   IMPORT POPULATION GROWTH DATA\n",
    "pop_growth = pd.read_csv('population_growth.csv')\n",
    "\n",
    "def populate_pop_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        temp = pd.DataFrame([[datetime.date(int(pop_growth['year'][i]),int(1),20), float(data['pop'][i])]],columns=['date','population'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "pop_df = pd.DataFrame(columns=['date','population'])\n",
    "pop_df = populate_pop_datetime(pop_growth, pop_df)\n",
    "\n",
    "###   SLICE ALL DFS TO 1990\n",
    "swe_monthly = swe_df.loc[swe_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "precip_monthly = precip_df.loc[precip_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "lk_lvl_monthly = lk_lvl_df.loc[lk_lvl_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "gdp_annual = gdp_df.loc[gdp_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "pop_annual = pop_df.loc[pop_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "\n",
    "#reset indexes\n",
    "swe_monthly = swe_monthly.reset_index(drop=True)\n",
    "precip_monthly = precip_monthly.reset_index(drop=True)\n",
    "lk_lvl_monthly = lk_lvl_monthly.reset_index(drop=True)\n",
    "gdp_annual = gdp_annual.reset_index(drop=True)\n",
    "pop_annual = pop_annual.reset_index(drop=True)\n",
    "\n",
    "###    ANNUAL INFORMATION FOR MONTHLY DATA\n",
    "\n",
    "#function to sum months to get annual data from monthly data\n",
    "def make_annual_sum(df, str, df_annual):\n",
    "    curr_year = 1990\n",
    "    curr_inches = 0\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].year)==curr_year):\n",
    "            curr_inches += df[str][i]\n",
    "        elif(month>12 or i == len(df)):\n",
    "            temp = pd.DataFrame([[datetime.date(int(curr_year),int(1),20), float(curr_inches)]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "            curr_year += 1\n",
    "            curr_inches = 0\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "swe_annual = pd.DataFrame(columns=['date','swe'])\n",
    "swe_annual = make_annual_sum(swe_monthly, 'swe', swe_annual)\n",
    "\n",
    "precip_annual = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_annual = make_annual_sum(precip_monthly, 'precipitation', precip_annual)\n",
    "\n",
    "def make_annual_lake_level(df, str, df_annual):\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].month)==1):\n",
    "            temp = pd.DataFrame([[df['date'][i], float(df[str][i])]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "lk_lvl_annual = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_annual = make_annual_lake_level(lk_lvl_monthly, 'lake_level', lk_lvl_annual)\n",
    "\n",
    "\n",
    "###    CLEAN DATA\n",
    "\n",
    "#monthly\n",
    "swe_monthly \n",
    "precip_monthly \n",
    "lk_lvl_monthly \n",
    "\n",
    "#annual\n",
    "gdp_annual \n",
    "pop_annual\n",
    "precip_annual\n",
    "lk_lvl_annual \n",
    "swe_annual\n",
    "\n",
    "###    REGRESSION ANALYSIS\n",
    "\n",
    "final = gdp_annual.merge(pop_annual, on = 'date', how = 'outer').merge(precip_annual, on = 'date', how = 'outer').merge(lk_lvl_annual, on = 'date', how = 'outer').merge(swe_annual, on = 'date', how = 'outer')\n",
    "final['ln_gdp'] = np.log(final['gdp'])\n",
    "final['ln_pop'] = np.log(final['population'])\n",
    "final['ln_precip'] = np.log(final['precipitation'])\n",
    "final['ln_lk_lvl'] = np.log(final['lake_level'])\n",
    "final['ln_swe'] = np.log(final['swe'])\n",
    "final['gdp_sq'] = np.square(final['gdp'])\n",
    "final['ln_gdp_sq'] = np.log(final['gdp_sq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>lake_level</td>    <th>  R-squared:         </th> <td>   0.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Apr 2023</td> <th>  Prob (F-statistic):</th> <td>1.21e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:41:39</td>     <th>  Log-Likelihood:    </th> <td> -44.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    25</td>      <th>  AIC:               </th> <td>   99.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   105.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>     <td> 4226.9539</td> <td>    5.709</td> <td>  740.349</td> <td> 0.000</td> <td> 4215.044</td> <td> 4238.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>swe</th>           <td>   -0.0413</td> <td>    0.019</td> <td>   -2.146</td> <td> 0.044</td> <td>   -0.081</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>precipitation</th> <td>   -0.1288</td> <td>    0.114</td> <td>   -1.132</td> <td> 0.271</td> <td>   -0.366</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>population</th>    <td>-1.093e-05</td> <td> 2.35e-06</td> <td>   -4.658</td> <td> 0.000</td> <td>-1.58e-05</td> <td>-6.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gdp_sq</th>        <td> 1.075e-10</td> <td>    7e-11</td> <td>    1.537</td> <td> 0.140</td> <td>-3.84e-11</td> <td> 2.53e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.058</td> <th>  Durbin-Watson:     </th> <td>   0.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.589</td> <th>  Jarque-Bera (JB):  </th> <td>   0.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.139</td> <th>  Prob(JB):          </th> <td>   0.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.141</td> <th>  Cond. No.          </th> <td>3.77e+11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.77e+11. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:             lake_level   R-squared:                       0.794\n",
       "Model:                            OLS   Adj. R-squared:                  0.753\n",
       "Method:                 Least Squares   F-statistic:                     19.31\n",
       "Date:                Wed, 19 Apr 2023   Prob (F-statistic):           1.21e-06\n",
       "Time:                        10:41:39   Log-Likelihood:                -44.569\n",
       "No. Observations:                  25   AIC:                             99.14\n",
       "Df Residuals:                      20   BIC:                             105.2\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "Intercept      4226.9539      5.709    740.349      0.000    4215.044    4238.864\n",
       "swe              -0.0413      0.019     -2.146      0.044      -0.081      -0.001\n",
       "precipitation    -0.1288      0.114     -1.132      0.271      -0.366       0.108\n",
       "population    -1.093e-05   2.35e-06     -4.658      0.000   -1.58e-05   -6.03e-06\n",
       "gdp_sq         1.075e-10      7e-11      1.537      0.140   -3.84e-11    2.53e-10\n",
       "==============================================================================\n",
       "Omnibus:                        1.058   Durbin-Watson:                   0.748\n",
       "Prob(Omnibus):                  0.589   Jarque-Bera (JB):                0.848\n",
       "Skew:                           0.139   Prob(JB):                        0.654\n",
       "Kurtosis:                       2.141   Cond. No.                     3.77e+11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.77e+11. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "first_reg = sm.ols('lake_level ~ swe + precipitation + population + gdp_sq', data = final).fit()\n",
    "first_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>ln_lk_lvl</td>    <th>  R-squared:         </th> <td>   0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.1273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 19 Apr 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.724</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:40:33</td>     <th>  Log-Likelihood:    </th> <td>  182.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>  -360.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    30</td>      <th>  BIC:               </th> <td>  -358.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    8.3426</td> <td>    0.002</td> <td> 4940.548</td> <td> 0.000</td> <td>    8.339</td> <td>    8.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ln_swe</th>    <td>   -0.0002</td> <td>    0.000</td> <td>   -0.357</td> <td> 0.724</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.641</td> <th>  Durbin-Watson:     </th> <td>   0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.440</td> <th>  Jarque-Bera (JB):  </th> <td>   1.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.362</td> <th>  Prob(JB):          </th> <td>   0.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.271</td> <th>  Cond. No.          </th> <td>    47.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              ln_lk_lvl   R-squared:                       0.004\n",
       "Model:                            OLS   Adj. R-squared:                 -0.029\n",
       "Method:                 Least Squares   F-statistic:                    0.1273\n",
       "Date:                Wed, 19 Apr 2023   Prob (F-statistic):              0.724\n",
       "Time:                        10:40:33   Log-Likelihood:                 182.45\n",
       "No. Observations:                  32   AIC:                            -360.9\n",
       "Df Residuals:                      30   BIC:                            -358.0\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      8.3426      0.002   4940.548      0.000       8.339       8.346\n",
       "ln_swe        -0.0002      0.000     -0.357      0.724      -0.001       0.001\n",
       "==============================================================================\n",
       "Omnibus:                        1.641   Durbin-Watson:                   0.571\n",
       "Prob(Omnibus):                  0.440   Jarque-Bera (JB):                1.407\n",
       "Skew:                           0.362   Prob(JB):                        0.495\n",
       "Kurtosis:                       2.271   Cond. No.                         47.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reg = sm.ols('ln_lk_lvl ~ ln_swe ', data = final).fit()\n",
    "final_reg.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
