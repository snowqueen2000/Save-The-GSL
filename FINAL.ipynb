{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Members \n",
    "<br>\n",
    "Audrey Pohl | u0497618 | u0497618@utah.edu\n",
    "<br>\n",
    "Clarissa Seebhom | u0575630 | u0575630@utah.edu\n",
    "<br>\n",
    "Joseph Wirthlin | u0936690 | u0936690@utah.edu\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Motivation \n",
    "\n",
    "Our group collectively enjoys outdoor activities in Salt Lake City. We ski, climb, hike, and camp in the wilderness surrounding Salt Lake City. For this reason, we value the Great Salt Lake for its contribution to the local ecosystem. With increased snowfall this winter, we are curious how much it will impact the level of the Salt Lake. Snowfall isnâ€™t the only factor that can impact the water level. We also will analyze the factors of rainfall, total precipitation, and water usage. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Objectives\n",
    "\n",
    "Do rainfall and snowfall have different effects on water levels of the GSL? Does precipitation or human factors have more of an impact on water levels of the GSL?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "There are four sets of data that we are using for this project, precipitation, snow water equivalent, gross domestic product (GDP), and population growth. The precipitation data for the Salt Lake Metropolitan Area comes from the National Weather Service. Snow water equivalent (SWE) is the amount of water in the Utah snowpack from the US Department of Agriculture. GDP comes from the Federal Reserve Economic Data (FRED) database. FRED is a reputable, government source that posts economic data for the entire country. The population data originally comes from the US Census Bureau but was condensed and simplified by MacroTrends, a website that reliably simplifies large datasets. \n",
    "\n",
    "Precipitation - https://www.weather.gov/wrh/climate?wfo=slc <br>\n",
    "SWE -  https://www.nrcs.usda.gov/Internet/WCIS/AWS_PLOTS/basinCharts/POR/WTEQ/assocHUCut3/state_of_utah.html <br>\n",
    "GDP - https://fred.stlouisfed.org/series/UTNGSP <br>\n",
    "Population data - https://www.macrotrends.net/states/utah/population#:~:text=The%20population%20of%20Utah%20in,a%201.53%25%20increase%20from%202018. <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data to be cleaned and processed:\n",
    "\n",
    "    - lake level (elevation) [m]\n",
    "    - snow water equivalent (snowfall) [inches]\n",
    "    - precipitation (salt lake metropolitan area) [inches]\n",
    "    - gdp of utah (proxy for commercial water usage) [dollars]\n",
    "    - population of utah (proxy for individual water usage) [people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd \n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT LAKE LEVEL DATA \n",
    "lake_level = pd.read_csv('monthly', sep = '\\t', comment = '#') \n",
    "\n",
    "#function to convert lake data to datetime\n",
    "def populate_lake_datetime(data, df):\n",
    "    for i in range(len(data)-1):\n",
    "        temp = pd.DataFrame([[datetime.date(int(data['year_nu'][i+1]),int(data['month_nu'][i+1]),20), float(data['mean_va'][i+1])]],columns=['date','lake_level'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "lk_lvl_df = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_df = populate_lake_datetime(lake_level, lk_lvl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT SNOW WATER EQUIVALENT DATA (inches)\n",
    "snow_water = pd.read_csv('state_of_utah_snow_water.csv') # snow water equivalent\n",
    "\n",
    "#find the mean swe per month\n",
    "october_snow = snow_water.loc[:30].mean()\n",
    "november_snow = snow_water.loc[31:60].mean()\n",
    "december_snow = snow_water.loc[61:91].mean()\n",
    "january_snow = snow_water.loc[92:122].mean()\n",
    "february_snow = snow_water.loc[124:151].mean()\n",
    "march_snow = snow_water.loc[152:182].mean()\n",
    "april_snow = snow_water.loc[183:212].mean()\n",
    "may_snow = snow_water.loc[213:243].mean()\n",
    "june_snow = snow_water.loc[244:273].mean()\n",
    "july_snow = snow_water.loc[274:304].mean()\n",
    "august_snow = snow_water.loc[305:334].mean()\n",
    "september_snow = snow_water.loc[335:365].mean()\n",
    "\n",
    "#function to convert swe data to date time\n",
    "    #one piece of data per month that is on the 20th\n",
    "def populate_snow_datetime(data, month, start_year, df):\n",
    "    for i in range(len(data)-10):\n",
    "        temp = pd.DataFrame([[datetime.date(start_year+i,month,20), data[i]]],columns=['date','swe'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#populate into a data frame\n",
    "swe_df = pd.DataFrame(columns=['date','swe'])\n",
    "\n",
    "swe_df = populate_snow_datetime(october_snow, 10, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(november_snow, 11, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(december_snow, 12, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(january_snow, 1, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(february_snow, 2, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(march_snow, 3, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(april_snow, 4, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(may_snow, 5, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(june_snow, 6, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(july_snow, 7, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(august_snow, 8, 1981, swe_df)\n",
    "swe_df = populate_snow_datetime(september_snow, 9, 1981, swe_df)\n",
    "\n",
    "swe_df = swe_df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT PRECIPITATION DATA (inches)\n",
    "precipitation = pd.read_csv('precipitation_data.csv')\n",
    "\n",
    "#function to convert pecipitation data to datetime\n",
    "def populate_precip_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(12):\n",
    "            month = (str)(j+1)\n",
    "            temp = pd.DataFrame([[datetime.date(int(data['Year'][i]),int(j+1),20), float(data[month][i])]],columns=['date','precipitation'])\n",
    "            df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "precip_df = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_df = populate_precip_datetime(precipitation, precip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT GDP DATA (some form of dollars)\n",
    "gdp = pd.read_csv('UTNGSP.csv')\n",
    "\n",
    "#function to convert gdp data to datetime\n",
    "def populate_gdp_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        year = data['DATE'][i][:4]\n",
    "        temp = pd.DataFrame([[datetime.date(int(year),int(1),20), float(data['UTNGSP'][i])]],columns=['date','gdp'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "gdp_df = pd.DataFrame(columns=['date','gdp'])\n",
    "gdp_df = populate_gdp_datetime(gdp, gdp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT POPULATION GROWTH DATA\n",
    "pop_growth = pd.read_csv('population_growth.csv')\n",
    "\n",
    "def populate_pop_datetime(data, df):\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        temp = pd.DataFrame([[datetime.date(int(pop_growth['year'][i]),int(1),20), float(data['pop'][i])]],columns=['date','population'])\n",
    "        df = pd.concat([df,temp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "pop_df = pd.DataFrame(columns=['date','population'])\n",
    "pop_df = populate_pop_datetime(pop_growth, pop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   SLICE ALL DFS TO 1990\n",
    "swe_monthly = swe_df.loc[swe_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "precip_monthly = precip_df.loc[precip_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "lk_lvl_monthly = lk_lvl_df.loc[lk_lvl_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "gdp_annual = gdp_df.loc[gdp_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "pop_annual = pop_df.loc[pop_df[\"date\"]>datetime.date(1990,1,1)]\n",
    "\n",
    "#reset indexes\n",
    "swe_monthly = swe_monthly.reset_index(drop=True)\n",
    "precip_monthly = precip_monthly.reset_index(drop=True)\n",
    "lk_lvl_monthly = lk_lvl_monthly.reset_index(drop=True)\n",
    "gdp_annual = gdp_annual.reset_index(drop=True)\n",
    "pop_annual = pop_annual.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    ANNUAL INFORMATION FOR MONTHLY DATA\n",
    "\n",
    "#function to sum months to get annual data from monthly data\n",
    "def make_annual_sum(df, str, df_annual):\n",
    "    curr_year = 1990\n",
    "    curr_inches = 0\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].year)==curr_year):\n",
    "            curr_inches += df[str][i]\n",
    "        elif(month>12 or i == len(df)):\n",
    "            temp = pd.DataFrame([[datetime.date(int(curr_year),int(1),20), float(curr_inches)]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "            curr_year += 1\n",
    "            curr_inches = 0\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "swe_annual = pd.DataFrame(columns=['date','swe'])\n",
    "swe_annual = make_annual_sum(swe_monthly, 'swe', swe_annual)\n",
    "\n",
    "precip_annual = pd.DataFrame(columns=['date','precipitation'])\n",
    "precip_annual = make_annual_sum(precip_monthly, 'precipitation', precip_annual)\n",
    "\n",
    "def make_annual_lake_level(df, str, df_annual):\n",
    "    month = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        month += 1\n",
    "        if(int(df['date'][i].month)==1):\n",
    "            temp = pd.DataFrame([[df['date'][i], float(df[str][i])]],columns=['date', str])\n",
    "            df_annual = pd.concat([df_annual,temp], ignore_index=True)\n",
    "\n",
    "    return df_annual\n",
    "\n",
    "lk_lvl_annual = pd.DataFrame(columns=['date','lake_level'])\n",
    "lk_lvl_annual = make_annual_lake_level(lk_lvl_monthly, 'lake_level', lk_lvl_annual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    CLEAN DATA\n",
    "\n",
    "#monthly\n",
    "swe_monthly \n",
    "precip_monthly \n",
    "lk_lvl_monthly \n",
    "\n",
    "#annual\n",
    "gdp_annual \n",
    "pop_annual\n",
    "precip_annual\n",
    "lk_lvl_annual \n",
    "swe_annual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Considerations\n",
    "\n",
    "We must confirm that the data we use will not cause harm to any stakeholders. To prevent harm we identified the primary stakeholders related to this research: Utah residents, the Utah government and policymakers, Utah agriculture, and Utah watershed-reliant businesses. In order to prevent causing harm to individual stakeholders, we decided to use water usage data on a state level so not to unintentionally blame any water users. This is because some historical research left certain stakeholders feeling as if they are receiving a disproportionate amount of the blame and are under increasing pressure to decrease water use as a result of that research. We want to avoid this, so our data will not break down any type of water user to protect those stakeholders from experiencing public scrutiny.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
